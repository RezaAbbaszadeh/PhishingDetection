{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filepath = \"features/url_features.csv\"  # Replace with your dataset file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['qty_._url', 'qty_-_url', 'qty_/_url', 'qty_?_url', 'qty_=_url',\n",
       "       'qty_@_url', 'qty_&_url', 'qty_tld_url', 'length_url', 'email_in_url',\n",
       "       'qty_._domain', 'qty_-_domain', 'qty_vowels_domain', 'domain_length',\n",
       "       'subdomain_level', 'qty_._directory', 'qty_-_directory',\n",
       "       'qty___directory', 'qty_/_directory', 'directory_length', 'qty_._file',\n",
       "       'qty_-_file', 'qty___file', 'file_length', 'qty_._params',\n",
       "       'qty___params', 'qty_=_params', 'qty_&_params', 'params_length',\n",
       "       'qty_params', 'num_dots', 'path_level', 'url_length', 'num_dash',\n",
       "       'num_dash_in_hostname', 'at_symbol', 'num_query_components',\n",
       "       'num_ampersand', 'num_numeric_chars', 'no_https', 'random_string',\n",
       "       'domain_in_subdomains', 'domain_in_paths', 'hostname_length',\n",
       "       'path_length', 'query_length', 'num_sensitive_words',\n",
       "       'at_symbol_in_url', 'prefix_suffix_in_domain', 'subdomain_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "df = df.loc[:, df.nunique() > 1]\n",
    "X = df.drop(columns=['rec_id', 'phishing'])\n",
    "y = df['phishing']\n",
    "\n",
    "# Apply SelectKBest with mutual information\n",
    "k = 50  # Number of top features to select\n",
    "selector = SelectKBest(mutual_info_classif, k=k)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "# Create a DataFrame with the selected features\n",
    "X_selected = pd.DataFrame(X_new, columns=selected_features)\n",
    "\n",
    "# Display the selected features\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 51)\n",
      "(80000, 51)\n",
      "linear qty_&_url [ 0.  3.  6. 10.]\n",
      "linear qty_=_directory [0. 1. 2.]\n",
      "linear qty___file [0. 3. 4. 8.]\n",
      "linear qty___params [ 0.  2.  3. 18.]\n",
      "linear qty_&_params [0. 3. 5. 9.]\n",
      "linear num_ampersand [ 0.  3.  6. 10.]\n",
      "linear domain_in_subdomains [0. 1. 2.]\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(filepath, n_bins=4):\n",
    "    \"\"\"\n",
    "    Loads the dataset and converts features into itemsets for association rule mining.\n",
    "    Binary features are converted to <feature_name>_1 or <feature_name>_0.\n",
    "    Numerical features are binned into ranges and converted to <feature_name>_<start_value>_<end_value>.\n",
    "    Assumes the column 'phishing' is the target class.\n",
    "    \n",
    "    Parameters:\n",
    "        filepath (str): Path to the CSV file.\n",
    "        n_bins (int): Number of bins for numerical features.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame where rows represent itemsets.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df[selected_features.tolist() + ['phishing']]\n",
    "    print(df.shape)\n",
    "    df = df.iloc[:, :]  # For testing on a smaller subset\n",
    "    print(df.shape)\n",
    "\n",
    "    itemsets = []\n",
    "\n",
    "    bins_dict = {}\n",
    "    for col in df.columns:\n",
    "        if col!='phishing' and (df[col].dtype in ['int64', 'float64']) and df[col].nunique() > 2:\n",
    "            df[col] = df[col].clip(lower=df[col].min(), \n",
    "                                               upper=df[col].quantile(0.999))\n",
    "            if df[col].nunique() <= 2:\n",
    "                continue\n",
    "            bins = pd.qcut(df[col], q=n_bins, duplicates='drop', retbins=True)[1]\n",
    "            if len(bins) < 3:\n",
    "                bins = pd.cut(df[col], \n",
    "                              bins=[df[col].min(), df[col].quantile(0.01), df[col].quantile(0.05), df[col].quantile(0.1), \n",
    "                                    df[col].quantile(0.2),df[col].quantile(0.4), df[col].quantile(0.99), \n",
    "                                    df[col].quantile(0.995), df[col].quantile(0.999), df[col].max()],\n",
    "                                       duplicates='drop', retbins=True)[1]\n",
    "                if len(bins) == 2:\n",
    "                    bins = np.insert(bins, 1, (bins[0]+bins[1])/2)\n",
    "                print('linear', col, bins)\n",
    "            bin_labels = [f\"{col}_{round(bins[i], 2)}_{round(bins[i+1], 2)}\" for i in range(len(bins) - 1)]\n",
    "            bins_dict[col] = {'bins': bins, 'labels': bin_labels}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index % 1000 == 0:\n",
    "            print(index)\n",
    "        itemset = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if df[col].dtype in ['int64', 'float64'] and df[col].nunique() > 2:\n",
    "                # Numerical feature: binning into ranges\n",
    "                bins = bins_dict[col]['bins']\n",
    "                bin_labels = bins_dict[col]['labels']\n",
    "                bin_idx = np.digitize(row[col], bins) - 1\n",
    "                bin_idx = min(bin_idx, len(bin_labels) - 1)  # Ensure index is within range\n",
    "                itemset.append(bin_labels[bin_idx])\n",
    "            else:\n",
    "                # Binary/categorical feature: encode as <feature_name>_value\n",
    "                itemset.append(f\"{col}_{int(row[col])}\")\n",
    "        \n",
    "        itemsets.append(itemset)\n",
    "\n",
    "    # Convert itemsets into a DataFrame for analysis\n",
    "    transactions_df = pd.DataFrame({'itemsets': itemsets})\n",
    "    return transactions_df\n",
    "\n",
    "# Load and preprocess data\n",
    "df = preprocess_data(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pattern_mining/preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[qty_._url_2.0_3.0, qty_-_url_0.0_1.0, qty_/_u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[qty_._url_3.0_23.0, qty_-_url_0.0_1.0, qty_/_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[qty_._url_3.0_23.0, qty_-_url_0.0_1.0, qty_/_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[qty_._url_3.0_23.0, qty_-_url_1.0_2.0, qty_/_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[qty_._url_3.0_23.0, qty_-_url_0.0_1.0, qty_/_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            itemsets\n",
       "0  [qty_._url_2.0_3.0, qty_-_url_0.0_1.0, qty_/_u...\n",
       "1  [qty_._url_3.0_23.0, qty_-_url_0.0_1.0, qty_/_...\n",
       "2  [qty_._url_3.0_23.0, qty_-_url_0.0_1.0, qty_/_...\n",
       "3  [qty_._url_3.0_23.0, qty_-_url_1.0_2.0, qty_/_...\n",
       "4  [qty_._url_3.0_23.0, qty_-_url_0.0_1.0, qty_/_..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pattern_mining/preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def mine_frequent_itemsets(df_itemsets, label, min_support=0.01):\n",
    "    \"\"\"\n",
    "    Mines frequent patterns for transactions containing 'phishing_1',\n",
    "    and appends 'phishing_1' to all resulting frequent itemsets.\n",
    "    \n",
    "    Parameters:\n",
    "        df_itemsets (pd.DataFrame): DataFrame with a column 'itemsets' containing lists of items.\n",
    "        min_support (float): Minimum support threshold for frequent itemsets.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Frequent itemsets including 'phishing_1' with their support values.\n",
    "    \"\"\"\n",
    "    # Filter transactions to only include those containing 'phishing_1'\n",
    "\n",
    "    random.seed = 42\n",
    "    f = list(selected_features)\n",
    "    random.shuffle(f)\n",
    "    features = [tuple(f[:25]), tuple(f[25:])]\n",
    "\n",
    "    all_frequent_itemsets = []\n",
    "    \n",
    "    for feature_set in features:\n",
    "    # Remove 'phishing_1' from each transaction (to avoid redundancy in mining)\n",
    "        transactions = df_itemsets['itemsets']\n",
    "        filtered_transactions = [t for t in transactions if label in t]\n",
    "        filtered_transactions = [\n",
    "            [item for item in t if (item != label and item.startswith(feature_set))] for t in filtered_transactions\n",
    "        ]\n",
    "        \n",
    "        # Convert transactions into a one-hot encoded DataFrame\n",
    "        te = TransactionEncoder()\n",
    "        one_hot = te.fit_transform(filtered_transactions)\n",
    "        one_hot_df = pd.DataFrame(one_hot, columns=te.columns_)\n",
    "        \n",
    "        # Mine frequent patterns from filtered transactions\n",
    "        frequent_itemsets = fpgrowth(one_hot_df, min_support=min_support, use_colnames=True)\n",
    "        # frequent_itemsets = apriori(one_hot_df, min_support=min_support, use_colnames=True)\n",
    "        \n",
    "        # Append 'phishing_1' to all itemsets\n",
    "        # frequent_itemsets['itemsets'] = frequent_itemsets['itemsets'].apply(lambda x: x | {label})\n",
    "        all_frequent_itemsets.append(frequent_itemsets)\n",
    "        print('done')\n",
    "        \n",
    "    combined_frequent_itemsets = pd.concat(all_frequent_itemsets, ignore_index=True)\n",
    "    print(f\"Frequent itemsets including {label}:\")\n",
    "    print(combined_frequent_itemsets)\n",
    "    \n",
    "    return combined_frequent_itemsets\n",
    "\n",
    "\n",
    "min_support = 0.01  # Adjust minimum support threshold\n",
    "# Mine frequent itemsets\n",
    "frequent_itemset_0 = mine_frequent_itemsets(df, 'phishing_0.0', min_support)\n",
    "frequent_itemset_1 = mine_frequent_itemsets(df, 'phishing_1.0', min_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>(phishing_1, query_length_0.0_772.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>(phishing_1, num_underscore_0.0_23.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>(num_query_components_0.0_11.0, phishing_1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>(phishing_1, num_percent_0.0_34.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>(num_ampersand_0.0_10.0, phishing_1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65530</th>\n",
       "      <td>0.915663</td>\n",
       "      <td>(num_query_components_0.0_11.0, ip_address_0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65531</th>\n",
       "      <td>0.915663</td>\n",
       "      <td>(ip_address_0, query_length_0.0_772.0, num_has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65532</th>\n",
       "      <td>0.915663</td>\n",
       "      <td>(num_query_components_0.0_11.0, ip_address_0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65533</th>\n",
       "      <td>0.915663</td>\n",
       "      <td>(num_query_components_0.0_11.0, ip_address_0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65534</th>\n",
       "      <td>0.915663</td>\n",
       "      <td>(ip_address_0, at_symbol_in_url_0, ip_address_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        support                                           itemsets\n",
       "0      1.000000               (phishing_1, query_length_0.0_772.0)\n",
       "1      1.000000              (phishing_1, num_underscore_0.0_23.0)\n",
       "2      1.000000        (num_query_components_0.0_11.0, phishing_1)\n",
       "3      1.000000                 (phishing_1, num_percent_0.0_34.0)\n",
       "4      1.000000               (num_ampersand_0.0_10.0, phishing_1)\n",
       "...         ...                                                ...\n",
       "65530  0.915663  (num_query_components_0.0_11.0, ip_address_0, ...\n",
       "65531  0.915663  (ip_address_0, query_length_0.0_772.0, num_has...\n",
       "65532  0.915663  (num_query_components_0.0_11.0, ip_address_0, ...\n",
       "65533  0.915663  (num_query_components_0.0_11.0, ip_address_0, ...\n",
       "65534  0.915663  (ip_address_0, at_symbol_in_url_0, ip_address_...\n",
       "\n",
       "[65535 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "        support                                           itemsets\n",
      "32767  0.915663  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "49151  0.942436  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "57343  0.915663  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "61439  0.918340  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "63487  0.918340  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "64511  0.917001  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "65524  0.917001  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "65525  0.915663  (num_query_components_0.0_11.0, query_length_0...\n",
      "65526  0.915663  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "65527  0.915663  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "65528  0.915663  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "65529  0.915663  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "65530  0.915663  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "65531  0.915663  (ip_address_0, query_length_0.0_772.0, num_has...\n",
      "65532  0.915663  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "65533  0.915663  (num_query_components_0.0_11.0, ip_address_0, ...\n",
      "65534  0.915663  (ip_address_0, at_symbol_in_url_0, ip_address_...\n"
     ]
    }
   ],
   "source": [
    "filtered = frequent_itemsets[frequent_itemsets['itemsets'].apply(len) > 15]\n",
    "print(len(filtered))\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate Association Rules with Consequent (phishing=1)\n",
    "def generate_association_rules(df, frequent_itemsets, label, min_threshold=0.5):\n",
    "\n",
    "    transactions = df['itemsets']\n",
    "    te = TransactionEncoder()\n",
    "    one_hot = te.fit_transform(transactions)\n",
    "    one_hot_df = pd.DataFrame(one_hot, columns=te.columns_)\n",
    "\n",
    "\n",
    "    # Calculate support for antecedent and union\n",
    "    total_transactions = len(transactions)\n",
    "    results = []\n",
    "\n",
    "    for itemset in frequent_itemsets:\n",
    "        # Convert itemset to list\n",
    "        itemset_list = list(itemset)\n",
    "        \n",
    "        # Calculate support of antecedent\n",
    "        antecedent_support = transactions[itemset_list].all(axis=1).sum() / total_transactions\n",
    "        \n",
    "        # Calculate support of (antecedent ∪ label)\n",
    "        union_support = transactions[itemset_list + [label]].all(axis=1).sum() / total_transactions\n",
    "        \n",
    "        # Calculate confidence\n",
    "        confidence = union_support / antecedent_support if antecedent_support > 0 else 0\n",
    "        \n",
    "        if confidence > min_threshold:\n",
    "            results.append({\n",
    "                \"Itemset\": itemset,\n",
    "                \"Antecedent Support\": antecedent_support,\n",
    "                \"Union Support\": union_support,\n",
    "                \"Confidence\": confidence\n",
    "            })\n",
    "\n",
    "    # Display results\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "    return rules\n",
    "\n",
    "\n",
    "rules = generate_association_rules(df, frequent_itemset_0, 'phishing_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_support = 0.1  # Adjust minimum support threshold\n",
    "min_confidence = 0.7  # Adjust minimum confidence threshold\n",
    "\n",
    "# Mine frequent itemsets\n",
    "frequent_itemsets = mine_frequent_itemsets(df, min_support)\n",
    "\n",
    "# Generate association rules\n",
    "rules = generate_association_rules(frequent_itemsets, min_confidence)\n",
    "\n",
    "# Save results\n",
    "frequent_itemsets.to_csv(\"frequent_itemsets.csv\", index=False)\n",
    "rules.to_csv(\"association_rules_phishing.csv\", index=False)\n",
    "print(\"Results saved to 'frequent_itemsets.csv' and 'association_rules_phishing.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
